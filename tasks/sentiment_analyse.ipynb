{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roby290597/nlp_exercise/blob/main/bundestags_reden_analyse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "extr hinzugefügt zu sys.path: c:\\Users\\brand\\OneDrive\\Desktop\\AI_Selbststudium\\ML_little\\bundestag_reden\\extr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "oliverguhr/german-sentiment-bert loaded.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "extr = next((str(p/\"extr\") for p in [Path.cwd()] + list(Path.cwd().parents) if (p/\"extr\").is_dir()), None)\n",
        "if extr and extr not in sys.path:\n",
        "    sys.path.insert(0, extr)\n",
        "print(\"extr hinzugefügt zu sys.path:\", extr)\n",
        "\n",
        "\n",
        "\n",
        "from redner_extraction import extract_speeches, extract_all_speakers, get_average_sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcL9PYxNY8zK"
      },
      "outputs": [],
      "source": [
        "#from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brand\\OneDrive\\Desktop\\AI_Selbststudium\\ML_little\\bundestag_reden\\tasks\n",
            "Parent-Dir hinzugefügt zu sys.path: c:\\Users\\brand\\OneDrive\\Desktop\\AI_Selbststudium\\ML_little\\bundestag_reden\n",
            "['.git', '.gitignore', 'config.yaml', 'extr', 'fine_tune_summarization.py', 'GPT_fine_tune_summarization.py', 'README.md', 'requirements.txt', 'tasks', 'ToDo.txt', 'web_ui']\n"
          ]
        }
      ],
      "source": [
        "# Füge Parent-Dir von `extr` zu sys.path hinzu, damit z.B. config.yaml gefunden werden kann\n",
        "\n",
        "print(os.getcwd())\n",
        "parent_dir = str(Path(extr).parent)\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)\n",
        "print(\"Parent-Dir hinzugefügt zu sys.path:\", parent_dir)\n",
        "\n",
        "print(os.listdir(parent_dir))\n",
        "\n",
        "\n",
        "import yaml\n",
        "# Load YAML data\n",
        "with open(parent_dir + \"/config.yaml\", \"r\") as file:\n",
        "    data = yaml.safe_load(file)  # safe_load prevents arbitrary code execution\n",
        "\n",
        "# Modify data (optional)\n",
        "url = data['data'][\"xml_file\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Web Scraping: Neuestes Plenarprotokoll des Bundestags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['vertrieb', 'herstellung', 'sitzung-ort', 'herausgeber', 'issn', 'wahlperiode', 'sitzung-nr', 'sitzung-datum', 'sitzung-start-uhrzeit', 'sitzung-ende-uhrzeit', 'sitzung-naechste-datum', 'start-seitennr']\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "# XML in einen Tree parsen\n",
        "tree = ET.ElementTree(ET.fromstring(response.content))\n",
        "\n",
        "# Wurzel-Element abrufen\n",
        "root = tree.getroot()\n",
        "\n",
        "print(root.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlzCruHSZDml",
        "outputId": "53a5275d-598d-443c-f80c-f7bf7a3c51d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vertrieb': 'Bundesanzeiger Verlag GmbH, Postfach 1 0 05 34, 50445 Köln, Telefon (02 21) 97 66 83 40, Fax (02 21) 97 66 83 44, www.bundesanzeiger-verlag.de',\n",
              " 'herstellung': 'H. Heenemann GmbH  Co. KG, Buch- und Offsetdruckerei, Bessemerstraße 83–91, 12103 Berlin, www.heenemann-druck.de',\n",
              " 'sitzung-ort': 'Berlin',\n",
              " 'herausgeber': 'Deutscher Bundestag',\n",
              " 'issn': '0722-7980',\n",
              " 'wahlperiode': '21',\n",
              " 'sitzung-nr': '32',\n",
              " 'sitzung-datum': '10.10.2025',\n",
              " 'sitzung-start-uhrzeit': '09:00',\n",
              " 'sitzung-ende-uhrzeit': '15:13',\n",
              " 'sitzung-naechste-datum': '15.10.2025',\n",
              " 'start-seitennr': '3443'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root.attrib # Alle Attribute des Wurzel-Elements anzeigen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6CHLV8cZDp2",
        "outputId": "95be7714-6664-4d74-831b-8e8d7fe56382"
      },
      "source": [
        "### Extract all speeches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doppelte Rede: Mandy Eißing\n",
            "Doppelte Rede: Peter Bohnhof\n",
            "Doppelte Rede: Mahmut Özdemir\n",
            "Doppelte Rede: Klaus Wiener\n",
            "Doppelte Rede: Mirze Edis\n"
          ]
        }
      ],
      "source": [
        "reden = {}\n",
        "redner = []\n",
        "# Beispiel-Ausgabe\n",
        "for speech in extract_speeches(root):\n",
        "    #print(f\"Redner: {speech['name']}\\nRede: {speech['text'][:200]}...\\n\")\n",
        "    if speech['name'] not in redner:\n",
        "        redner.append(speech['name'])\n",
        "        reden[speech['name']] = speech['text']\n",
        "    else:\n",
        "        print(\"Doppelte Rede:\", speech['name'])\n",
        "        reden[speech['name']] += \"\\n Nächste Rede:\" + speech['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'vorname': 'Bärbel', 'nachname': 'Bas', 'fraktion': '', 'rolle_kurz': 'Bundesministerin BMAS'}\n",
            "{'vorname': 'Hans-Jürgen', 'nachname': 'Goßner', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Wilfried', 'nachname': 'Oellers', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Ricarda', 'nachname': 'Lang', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Pascal', 'nachname': 'Meiser', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Dagmar', 'nachname': 'Schmidt', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Peter', 'nachname': 'Bohnhof', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Sandra', 'nachname': 'Carstensen', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Armin', 'nachname': 'Grau', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Nora', 'nachname': 'Seitz', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Leif-Erik', 'nachname': 'Holm', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Peter', 'nachname': 'Aumer', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Jan', 'nachname': 'Dieren', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Elisabeth', 'nachname': 'Winkelmeier-Becker', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Tobias Matthias', 'nachname': 'Peterka', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Axel', 'nachname': 'Müller', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Lena', 'nachname': 'Gumnior', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Mahmut', 'nachname': 'Özdemir', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Luke', 'nachname': 'Hoß', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Christian', 'nachname': 'Moser', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Rainer', 'nachname': 'Galla', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Maja', 'nachname': 'Wallstein', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Helge', 'nachname': 'Limburg', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Johannes', 'nachname': 'Wiegelmann', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Knuth', 'nachname': 'Meyer-Soltau', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'David', 'nachname': 'Preisendanz', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Wolfram', 'nachname': 'Weimer', 'fraktion': '', 'rolle_kurz': 'Staatsminister BK'}\n",
            "{'vorname': 'Götz', 'nachname': 'Frömming', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Elisabeth', 'nachname': 'Kaiser', 'fraktion': '', 'rolle_kurz': 'Staatsministerin BMF'}\n",
            "{'vorname': 'Katrin', 'nachname': 'Göring-Eckardt', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Mandy', 'nachname': 'Eißing', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Ottilie', 'nachname': 'Klein', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Sven', 'nachname': 'Wendorf', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Franziska', 'nachname': 'Kersten', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Sepp', 'nachname': 'Müller', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Wolfgang', 'nachname': 'Dahler', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Birgit', 'nachname': 'Bessin', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Felix', 'nachname': 'Döring', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Misbah', 'nachname': 'Khan', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Konrad', 'nachname': 'Körner', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Anna', 'nachname': 'Rathert', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Reem', 'nachname': 'Alabali Radovan', 'fraktion': '', 'rolle_kurz': 'Bundesministerin BMZ'}\n",
            "{'vorname': 'Rocco', 'nachname': 'Kever', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Nicolas', 'nachname': 'Zippelius', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Claudia', 'nachname': 'Roth', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Mirze', 'nachname': 'Edis', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Klaus', 'nachname': 'Wiener', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Johann', 'nachname': 'Martel', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Thomas', 'nachname': 'Bareiß', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Matthias', 'nachname': 'Gastel', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Michael', 'nachname': 'Donth', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Ulrich', 'nachname': 'von Zons', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Anja', 'nachname': 'Troff-Schaffarzyk', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Luigi', 'nachname': 'Pantisano', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Henning', 'nachname': 'Rehbaum', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Maximilian', 'nachname': 'Kneller', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Jakob', 'nachname': 'Blankenburg', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Günter', 'nachname': 'Baumgartner', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Katherina', 'nachname': 'Reiche', 'fraktion': '', 'rolle_kurz': 'Bundesministerin BMWK'}\n",
            "{'vorname': 'Steffen', 'nachname': 'Kotré', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Daniel', 'nachname': 'Walter', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Katrin', 'nachname': 'Uhlig', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Jörg', 'nachname': 'Cezanne', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Tilman', 'nachname': 'Kuban', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Raimond', 'nachname': 'Scheirich', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Agnes', 'nachname': 'Conrad', 'fraktion': 'Die Linke', 'rolle_kurz': ''}\n",
            "{'vorname': 'Andreas', 'nachname': 'Lenz', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Andreas', 'nachname': 'Audretsch', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Dirk', 'nachname': 'Brandes', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Bernd', 'nachname': 'Rützel', 'fraktion': 'SPD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Michael', 'nachname': 'Kellner', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN', 'rolle_kurz': ''}\n",
            "{'vorname': 'Lars', 'nachname': 'Rohwer', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n",
            "{'vorname': 'Gerrit', 'nachname': 'Huy', 'fraktion': 'AfD', 'rolle_kurz': ''}\n",
            "{'vorname': 'Vanessa', 'nachname': 'Zobel', 'fraktion': 'CDU/CSU', 'rolle_kurz': ''}\n"
          ]
        }
      ],
      "source": [
        "# Extrahiere alle Redner und ihre Fraktionen\n",
        "redner_party = []\n",
        "all_speakers = extract_all_speakers(root)\n",
        "for speaker in all_speakers:\n",
        "    print(speaker)\n",
        "    redner_party.append((speaker['vorname'] + \" \" + speaker['nachname'], speaker['fraktion']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vektor-Länge: 309\n",
            "\n",
            "Top-Terme (mit TF-IDF-Werten):\n",
            "und: 0.3627\n",
            "auch: 0.3174\n",
            "wir: 0.2947\n",
            "die: 0.2720\n",
            "der: 0.2267\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import numpy as np\n",
        "\n",
        "# # TF-IDF-Vektor für die Rede von Bärbel Bas erzeugen\n",
        "\n",
        "# # Rede aus bestehendem Dictionary verwenden\n",
        "# text = reden.get(\"Bärbel Bas\", \"\")\n",
        "# if not text:\n",
        "#     raise ValueError(\"Keine Rede für 'Bärbel Bas' im Dictionary 'reden' gefunden.\")\n",
        "\n",
        "# # Deutsches Stopword-Set nutzen, TF-IDF berechnen\n",
        "# vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "# X = vectorizer.fit_transform([text])  # Ergebnis als sparse matrix\n",
        "# baerbel_vec = X.toarray()[0]          # dichter Vektor\n",
        "\n",
        "# print(\"Vektor-Länge:\", baerbel_vec.shape[0])\n",
        "\n",
        "# # Wichtigste Terme anzeigen (Top 20)\n",
        "# feature_names = vectorizer.get_feature_names_out()\n",
        "# top_n = 5\n",
        "# top_idx = np.argsort(baerbel_vec)[::-1][:top_n]\n",
        "# print(\"\\nTop-Terme (mit TF-IDF-Werten):\")\n",
        "# for i in top_idx:\n",
        "#     print(f\"{feature_names[i]}: {baerbel_vec[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'negative', 'score': 0.9890553951263428}]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'label': 'negative', 'score': 0.9890553951263428, 'chunks_analyzed': 1}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_average_sentiment(\"Beifall\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'negative', 'score': 0.9890553951263428}]]\n"
          ]
        }
      ],
      "source": [
        "result = get_average_sentiment(\"Beifall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Durchschnittlicher Sentiment-Wert für 'Beifall': {'label': 'negative', 'score': 0.9890553951263428, 'chunks_analyzed': 1}\n"
          ]
        }
      ],
      "source": [
        "print(\"Durchschnittlicher Sentiment-Wert für 'Beifall':\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.9515988826751709}], [{'label': 'neutral', 'score': 0.9851915836334229}], [{'label': 'neutral', 'score': 0.9700415730476379}], [{'label': 'neutral', 'score': 0.9984058737754822}], [{'label': 'positive', 'score': 0.8976163268089294}], [{'label': 'positive', 'score': 0.8420059680938721}], [{'label': 'neutral', 'score': 0.8973153233528137}], [{'label': 'neutral', 'score': 0.9516069889068604}]]\n",
            "Redner: Dagmar Schmidt - Sentiment: neutral (Score: 0.9367, Chunks: 8)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.8554088473320007}], [{'label': 'negative', 'score': 0.8706532120704651}], [{'label': 'negative', 'score': 0.9875494241714478}], [{'label': 'neutral', 'score': 0.974997341632843}], [{'label': 'negative', 'score': 0.9931261539459229}], [{'label': 'neutral', 'score': 0.9718579053878784}], [{'label': 'negative', 'score': 0.9874758124351501}], [{'label': 'negative', 'score': 0.9443564414978027}], [{'label': 'negative', 'score': 0.8205112218856812}], [{'label': 'negative', 'score': 0.9944542050361633}], [{'label': 'negative', 'score': 0.9500529170036316}], [{'label': 'neutral', 'score': 0.9986435770988464}], [{'label': 'neutral', 'score': 0.9805420637130737}], [{'label': 'negative', 'score': 0.9890170097351074}], [{'label': 'negative', 'score': 0.9718104004859924}], [{'label': 'negative', 'score': 0.988864541053772}], [{'label': 'neutral', 'score': 0.9791924953460693}], [{'label': 'neutral', 'score': 0.9809142351150513}]]\n",
            "Redner: Peter Bohnhof - Sentiment: neutral (Score: 0.9577, Chunks: 18)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.896780252456665}], [{'label': 'neutral', 'score': 0.9614895582199097}], [{'label': 'neutral', 'score': 0.9873005747795105}], [{'label': 'neutral', 'score': 0.99996018409729}], [{'label': 'neutral', 'score': 0.9999157190322876}], [{'label': 'negative', 'score': 0.9560478925704956}], [{'label': 'neutral', 'score': 0.9910157918930054}], [{'label': 'positive', 'score': 0.9868419766426086}]]\n",
            "Redner: Sandra Carstensen - Sentiment: neutral (Score: 0.9724, Chunks: 8)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'negative', 'score': 0.9711220860481262}], [{'label': 'negative', 'score': 0.9688982963562012}], [{'label': 'negative', 'score': 0.9456377625465393}], [{'label': 'neutral', 'score': 0.8750700354576111}], [{'label': 'neutral', 'score': 0.9823201298713684}], [{'label': 'negative', 'score': 0.6478054523468018}], [{'label': 'neutral', 'score': 0.9806022644042969}]]\n",
            "Redner: Armin Grau - Sentiment: negative (Score: 0.9102, Chunks: 7)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.9853999018669128}], [{'label': 'neutral', 'score': 0.9810208082199097}], [{'label': 'neutral', 'score': 0.9901256561279297}], [{'label': 'neutral', 'score': 0.8967362642288208}], [{'label': 'neutral', 'score': 0.9654902815818787}], [{'label': 'negative', 'score': 0.9860677123069763}], [{'label': 'neutral', 'score': 0.9997407793998718}], [{'label': 'neutral', 'score': 0.8612605333328247}]]\n",
            "Redner: Nora Seitz - Sentiment: neutral (Score: 0.9582, Chunks: 8)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'positive', 'score': 0.9721506237983704}], [{'label': 'negative', 'score': 0.9568761587142944}], [{'label': 'negative', 'score': 0.9987974166870117}], [{'label': 'neutral', 'score': 0.9899660348892212}], [{'label': 'negative', 'score': 0.9905034303665161}], [{'label': 'negative', 'score': 0.9796802401542664}], [{'label': 'negative', 'score': 0.9798747897148132}], [{'label': 'negative', 'score': 0.9763636589050293}]]\n",
            "Redner: Leif-Erik Holm - Sentiment: positive (Score: 0.9805, Chunks: 8)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.9877540469169617}], [{'label': 'negative', 'score': 0.917214572429657}], [{'label': 'negative', 'score': 0.9844628572463989}], [{'label': 'negative', 'score': 0.9073118567466736}], [{'label': 'neutral', 'score': 0.9721102714538574}], [{'label': 'neutral', 'score': 0.9714223742485046}], [{'label': 'neutral', 'score': 0.9564524292945862}], [{'label': 'neutral', 'score': 0.9947583079338074}], [{'label': 'neutral', 'score': 0.9644082188606262}]]\n",
            "Redner: Peter Aumer - Sentiment: neutral (Score: 0.9618, Chunks: 9)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'negative', 'score': 0.742055356502533}], [{'label': 'negative', 'score': 0.9873186945915222}], [{'label': 'negative', 'score': 0.9888424873352051}], [{'label': 'neutral', 'score': 0.9847490191459656}], [{'label': 'neutral', 'score': 0.8147343397140503}], [{'label': 'negative', 'score': 0.9617829918861389}], [{'label': 'neutral', 'score': 0.9420498013496399}]]\n",
            "Redner: Jan Dieren - Sentiment: negative (Score: 0.9174, Chunks: 7)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'neutral', 'score': 0.9740859270095825}], [{'label': 'neutral', 'score': 0.9899219870567322}], [{'label': 'neutral', 'score': 0.9698415994644165}], [{'label': 'neutral', 'score': 0.9999222755432129}], [{'label': 'neutral', 'score': 0.9301505088806152}], [{'label': 'negative', 'score': 0.970898449420929}], [{'label': 'negative', 'score': 0.9809260368347168}], [{'label': 'negative', 'score': 0.9974910020828247}]]\n",
            "Redner: Elisabeth Winkelmeier-Becker - Sentiment: neutral (Score: 0.9767, Chunks: 8)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "[[{'label': 'negative', 'score': 0.9870665669441223}], [{'label': 'negative', 'score': 0.9592154622077942}], [{'label': 'negative', 'score': 0.9952487349510193}], [{'label': 'negative', 'score': 0.9661126136779785}], [{'label': 'negative', 'score': 0.9555462002754211}], [{'label': 'negative', 'score': 0.9943957328796387}], [{'label': 'neutral', 'score': 0.536959171295166}], [{'label': 'neutral', 'score': 0.98073410987854}], [{'label': 'negative', 'score': 0.9733924865722656}], [{'label': 'negative', 'score': 0.9636716842651367}], [{'label': 'negative', 'score': 0.5742666125297546}], [{'label': 'negative', 'score': 0.6435378789901733}], [{'label': 'neutral', 'score': 0.9538996815681458}], [{'label': 'negative', 'score': 0.9279093742370605}]]\n",
            "Redner: Tobias Matthias Peterka - Sentiment: negative (Score: 0.8866, Chunks: 14)\n",
            "SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, redner \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reden.keys()):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i >= \u001b[32m5\u001b[39m:\n\u001b[32m      3\u001b[39m         \u001b[38;5;66;03m#break\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         result = \u001b[43mget_average_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreden\u001b[49m\u001b[43m[\u001b[49m\u001b[43mredner\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRedner: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mredner\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Sentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mchunks_analyzed\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\AI_Selbststudium\\ML_little\\bundestag_reden\\extr\\redner_extraction.py:134\u001b[39m, in \u001b[36mget_average_sentiment\u001b[39m\u001b[34m(text, chunk_size)\u001b[39m\n\u001b[32m    132\u001b[39m     chunk = text[i:i + chunk_size]\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.strip():  \u001b[38;5;66;03m# Nur nicht-leere Chunks verarbeiten\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         result = \u001b[43msentiment_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m         sentiments.append(result)\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Scores extrahieren und Durchschnitt berechnen\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:168\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    135\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    170\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:199\u001b[39m, in \u001b[36mTextClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters:\n\u001b[32m    198\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1482\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1475\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1476\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1477\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1478\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1480\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1496\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1000\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    998\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1014\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:650\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    646\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:588\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    585\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    586\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:596\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:512\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "for i, redner in enumerate(reden.keys()):\n",
        "    if i >= 5:\n",
        "        #break\n",
        "        result = get_average_sentiment(reden[redner])\n",
        "        print(f\"Redner: {redner} - Sentiment: {result['label']} (Score: {result['score']:.4f}, Chunks: {result['chunks_analyzed']})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP50MDuGDSS8xYQVPOiGNTy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "job",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
